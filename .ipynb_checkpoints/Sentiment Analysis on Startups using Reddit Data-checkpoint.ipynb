{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Startups Using Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named colorbar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f78e475e6fd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2255\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.pyc\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.pyc\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3132\u001b[0;31m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3133\u001b[0m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/pylabtools.pyc\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backend'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pylab_helpers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named colorbar"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview: We compiled a list of companies that we are interested in exploring through Reddit data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "companies = pd.read_csv(\"companies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aetna']\n",
      "['ZocDoc']\n",
      "['Oscar']\n",
      "['Chartboost']\n",
      "['Wayfair']\n",
      "['Palantir Technologies']\n",
      "['Grubhub']\n",
      "['AppNexus']\n",
      "['Rivot']\n",
      "['Riot Games']\n",
      "['EA']\n",
      "['Yik Yak']\n",
      "['Misfit Wearables']\n",
      "['Gilt Groupe']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "body1 = []\n",
    "score1 = []\n",
    "company1 = []\n",
    "link1 = []\n",
    "date1 = []\n",
    "title1 = []\n",
    "\n",
    "for i in companies.values:\n",
    "    try:\n",
    "        r = praw.Reddit('Comment parser example by u/_Daimon_')\n",
    "        subreddit = r.get_subreddit(i[0]).get_top(limit = 40)\n",
    "        for k in subreddit:\n",
    "            body1.append(k.selftext)\n",
    "            score1.append(k.score)\n",
    "            company1.append(i[0])\n",
    "            link1.append(k.permalink)\n",
    "            date1.append(k.created_utc)\n",
    "            title1.append(k)\n",
    "            try:\n",
    "                comments = praw.helpers.flatten_tree(k.comments)\n",
    "                for j in comments:\n",
    "                    body1.append(j.body)\n",
    "                    score1.append(j.score)\n",
    "                    company1.append(i[0])\n",
    "                    link1.append(j.permalink)\n",
    "                    date1.append(j.created_utc)\n",
    "                    title1.append(k)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    except:\n",
    "        print i\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit = pd.DataFrame({'body':body1, 'score':score1, 'company':company1, 'link':link1, 'date':date1, 'title':title1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit.to_csv(\"comments1.csv\",  encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facebook\n",
      "Google\n",
      "Apple\n",
      "Amazon\n",
      "Uber\n",
      "Lyft\n",
      "Airbnb\n",
      "Fitbit\n",
      "Jawbone\n",
      "Microsoft\n",
      "Coursera\n",
      "Athenahealth\n",
      "Aetna\n",
      "Twitter\n",
      "Snapchat\n",
      "Dropbox\n",
      "Spotify\n",
      "ZocDoc\n",
      "Asana\n",
      "LinkedIn\n",
      "Oscar\n",
      "Chartboost\n",
      "Groupon\n",
      "Wayfair\n",
      "Instagram\n",
      "BuzzFeed\n",
      "Pinterest\n",
      "Laserfiche\n",
      "Palantir Technologies\n",
      "Kayak\n",
      "Grubhub\n",
      "Box\n",
      "Hubspot\n",
      "Square\n",
      "AppNexus\n",
      "Quora\n",
      "Vivint\n",
      "Rivot\n",
      "Riot Games\n",
      "EA\n",
      "Yik Yak\n",
      "Misfit Wearables\n",
      "DoorDash\n",
      "Birchbox\n",
      "Slack\n",
      "Tinder\n",
      "Gilt Groupe\n",
      "GitHub\n",
      "Zenefits\n",
      "Facebook\n",
      "Google\n",
      "Apple\n",
      "Amazon\n",
      "Uber\n",
      "Lyft\n",
      "Airbnb\n",
      "Fitbit\n",
      "Jawbone\n",
      "Microsoft\n",
      "Coursera\n",
      "Athenahealth\n",
      "Aetna\n",
      "Twitter\n",
      "Snapchat\n",
      "Dropbox\n",
      "Spotify\n",
      "ZocDoc\n",
      "Asana\n",
      "LinkedIn\n",
      "Oscar\n",
      "Chartboost\n",
      "Groupon\n",
      "Wayfair\n",
      "Instagram\n",
      "BuzzFeed\n",
      "Pinterest\n",
      "Laserfiche\n",
      "Palantir Technologies\n",
      "Kayak\n",
      "Grubhub\n",
      "Box\n",
      "Hubspot\n",
      "Square\n",
      "AppNexus\n",
      "Quora\n",
      "Vivint\n",
      "Rivot\n",
      "Riot Games\n",
      "EA\n",
      "Yik Yak\n",
      "Misfit Wearables\n",
      "DoorDash\n",
      "Birchbox\n",
      "Slack\n",
      "Tinder\n",
      "Gilt Groupe\n",
      "GitHub\n",
      "Zenefits\n",
      "Facebook\n",
      "Google\n",
      "Apple\n",
      "Amazon\n",
      "Uber\n",
      "Lyft\n",
      "Airbnb\n",
      "Fitbit\n",
      "Jawbone\n",
      "Microsoft\n",
      "Coursera\n",
      "Athenahealth\n",
      "Aetna\n",
      "Twitter\n",
      "Snapchat\n",
      "Dropbox\n",
      "Spotify\n",
      "ZocDoc\n",
      "Asana\n",
      "LinkedIn\n",
      "Oscar\n",
      "Chartboost\n",
      "Groupon\n",
      "Wayfair\n",
      "Instagram\n",
      "BuzzFeed\n",
      "Pinterest\n",
      "Laserfiche\n",
      "Palantir Technologies\n",
      "Kayak\n",
      "Grubhub\n",
      "Box\n",
      "Hubspot\n",
      "Square\n",
      "AppNexus\n",
      "Quora\n",
      "Vivint\n",
      "Rivot\n",
      "Riot Games\n",
      "EA\n",
      "Yik Yak\n",
      "Misfit Wearables\n",
      "DoorDash\n",
      "Birchbox\n",
      "Slack\n",
      "Tinder\n",
      "Gilt Groupe\n",
      "GitHub\n",
      "Zenefits\n",
      "Facebook\n",
      "Google\n",
      "Apple\n",
      "Amazon\n",
      "Uber\n",
      "Lyft\n",
      "Airbnb\n",
      "Fitbit\n",
      "Jawbone\n",
      "Microsoft\n",
      "Coursera\n",
      "Athenahealth\n",
      "Aetna\n",
      "Twitter\n",
      "Snapchat\n",
      "Dropbox\n",
      "Spotify\n",
      "ZocDoc\n",
      "Asana\n",
      "LinkedIn\n",
      "Oscar\n",
      "Chartboost\n",
      "Groupon\n",
      "Wayfair\n",
      "Instagram\n",
      "BuzzFeed\n",
      "Pinterest\n",
      "Laserfiche\n",
      "Palantir Technologies\n",
      "Kayak\n",
      "Grubhub\n",
      "Box\n",
      "Hubspot\n",
      "Square\n",
      "AppNexus\n",
      "Quora\n",
      "Vivint\n",
      "Rivot\n",
      "Riot Games\n",
      "EA\n",
      "Yik Yak\n",
      "Misfit Wearables\n",
      "DoorDash\n",
      "Birchbox\n",
      "Slack\n",
      "Tinder\n",
      "Gilt Groupe\n",
      "No data for Gilt Groupe\n",
      "GitHub\n",
      "No data for GitHub\n",
      "Zenefits\n",
      "No data for Zenefits\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "body2 = []\n",
    "score2 = []\n",
    "company2 = []\n",
    "link2 = []\n",
    "date2 = []\n",
    "title2 = []\n",
    "\n",
    "subs = ['tech', 'technology', 'business', 'startups']\n",
    "\n",
    "for sub in subs:\n",
    "    for comp in companies.values:\n",
    "        try:\n",
    "            r = praw.Reddit('Comment parser example by u/_Daimon_')\n",
    "            results = r.search(str('title:'+comp[0]), subreddit=sub, sort='top', limit=10)\n",
    "            print comp[0]\n",
    "            for result in results:\n",
    "                body2.append(result.selftext)\n",
    "                score2.append(result.score)\n",
    "                company2.append(comp[0])\n",
    "                link2.append(result.permalink)\n",
    "                date2.append(result.created_utc)\n",
    "                title2.append(result)\n",
    "                try:\n",
    "                    comments = praw.helpers.flatten_tree(result.comments)\n",
    "                    for comment in comments:\n",
    "                        body2.append(comment.body)\n",
    "                        score2.append(comment.score)\n",
    "                        company2.append(comp[0])\n",
    "                        link2.append(comment.permalink)\n",
    "                        date2.append(comment.created_utc)\n",
    "                        title2.append(result)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            print \"No data for \"+comp[0]\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relatedtopic = pd.DataFrame({'body':body2, 'score':score2, 'company':company2, 'link':link2, 'date':date2, 'title':title2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relatedtopic.to_csv(\"comments2.csv\",  encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = subreddit.dropna()\n",
    "relatedtopic = relatedtopic.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subreddit = subreddit[subreddit.body != '[deleted]']\n",
    "subreddit = subreddit[subreddit.body != '[removed]']\n",
    "\n",
    "relatedtopic = relatedtopic[relatedtopic.body != '[deleted]']\n",
    "relatedtopic = relatedtopic[relatedtopic.body != '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorialin/anaconda/lib/python2.7/site-packages/pandas/core/common.py:516: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask = arr == x\n"
     ]
    }
   ],
   "source": [
    "subreddit_links = subreddit[\"link\"]\n",
    "subreddit = subreddit.drop(\"link\", axis=1)\n",
    "\n",
    "relatedtopic_links = relatedtopic[\"link\"]\n",
    "relatedtopic = relatedtopic.drop(\"link\", axis=1)\n",
    "\n",
    "subreddit = subreddit.replace(r\"(?:\\@|https?\\://)\\S+\", \"\", regex=True)\n",
    "relatedtopic = relatedtopic.replace(r\"(?:\\@|https?\\://)\\S+\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit = pd.concat([subreddit, subreddit_links], axis=1)\n",
    "relatedtopic = pd.concat([relatedtopic, relatedtopic_links], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit = subreddit[subreddit.body != '']\n",
    "relatedtopic = relatedtopic[relatedtopic.body != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Think he is cool.</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1449757453</td>\n",
       "      <td>1</td>\n",
       "      <td>4 :: Zuckerberg: Facebook will fight to protec...</td>\n",
       "      <td>https://www.reddit.com/r/facebook/comments/3w8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ha what?</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1449774574</td>\n",
       "      <td>1</td>\n",
       "      <td>3 :: New VIRUS on facebook. Watch out!</td>\n",
       "      <td>https://www.reddit.com/r/facebook/comments/3w8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I have my own business. I want to gain more vi...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1449735484</td>\n",
       "      <td>2</td>\n",
       "      <td>2 :: Facebook Page Manager</td>\n",
       "      <td>https://www.reddit.com/r/facebook/comments/3w7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I use the Facebook Pages app, it makes it easy...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1449751515</td>\n",
       "      <td>2</td>\n",
       "      <td>2 :: Facebook Page Manager</td>\n",
       "      <td>https://www.reddit.com/r/facebook/comments/3w7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I added someone as a friend, and normally upon...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1449692692</td>\n",
       "      <td>2</td>\n",
       "      <td>2 :: What does it mean when there is no \"Frien...</td>\n",
       "      <td>https://www.reddit.com/r/facebook/comments/3w4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   company        date  \\\n",
       "1                                  Think he is cool.  Facebook  1449757453   \n",
       "3                                           ha what?  Facebook  1449774574   \n",
       "7  I have my own business. I want to gain more vi...  Facebook  1449735484   \n",
       "8  I use the Facebook Pages app, it makes it easy...  Facebook  1449751515   \n",
       "9  I added someone as a friend, and normally upon...  Facebook  1449692692   \n",
       "\n",
       "   score                                              title  \\\n",
       "1      1  4 :: Zuckerberg: Facebook will fight to protec...   \n",
       "3      1             3 :: New VIRUS on facebook. Watch out!   \n",
       "7      2                         2 :: Facebook Page Manager   \n",
       "8      2                         2 :: Facebook Page Manager   \n",
       "9      2  2 :: What does it mean when there is no \"Frien...   \n",
       "\n",
       "                                                link  \n",
       "1  https://www.reddit.com/r/facebook/comments/3w8...  \n",
       "3  https://www.reddit.com/r/facebook/comments/3w8...  \n",
       "7  https://www.reddit.com/r/facebook/comments/3w7...  \n",
       "8  https://www.reddit.com/r/facebook/comments/3w7...  \n",
       "9  https://www.reddit.com/r/facebook/comments/3w4...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone ever use it?  Have any input?\\n</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1414096002</td>\n",
       "      <td>56</td>\n",
       "      <td>765 :: 'Anti-Facebook' social network Ello bec...</td>\n",
       "      <td>https://www.reddit.com/r/tech/comments/2k4j3k/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've used it. Haven't logged in since checking...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1414163826</td>\n",
       "      <td>1</td>\n",
       "      <td>765 :: 'Anti-Facebook' social network Ello bec...</td>\n",
       "      <td>https://www.reddit.com/r/tech/comments/2k4j3k/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, and it's pretty damn good. Way less noise...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1414163143</td>\n",
       "      <td>1</td>\n",
       "      <td>765 :: 'Anti-Facebook' social network Ello bec...</td>\n",
       "      <td>https://www.reddit.com/r/tech/comments/2k4j3k/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was unusable. Couldn't even figure out what...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1414143061</td>\n",
       "      <td>1</td>\n",
       "      <td>765 :: 'Anti-Facebook' social network Ello bec...</td>\n",
       "      <td>https://www.reddit.com/r/tech/comments/2k4j3k/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I was on it but have since closed my account. ...</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>1414139361</td>\n",
       "      <td>1</td>\n",
       "      <td>765 :: 'Anti-Facebook' social network Ello bec...</td>\n",
       "      <td>https://www.reddit.com/r/tech/comments/2k4j3k/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body   company        date  \\\n",
       "1             Anyone ever use it?  Have any input?\\n  Facebook  1414096002   \n",
       "2  I've used it. Haven't logged in since checking...  Facebook  1414163826   \n",
       "3  Yes, and it's pretty damn good. Way less noise...  Facebook  1414163143   \n",
       "4  It was unusable. Couldn't even figure out what...  Facebook  1414143061   \n",
       "5  I was on it but have since closed my account. ...  Facebook  1414139361   \n",
       "\n",
       "   score                                              title  \\\n",
       "1     56  765 :: 'Anti-Facebook' social network Ello bec...   \n",
       "2      1  765 :: 'Anti-Facebook' social network Ello bec...   \n",
       "3      1  765 :: 'Anti-Facebook' social network Ello bec...   \n",
       "4      1  765 :: 'Anti-Facebook' social network Ello bec...   \n",
       "5      1  765 :: 'Anti-Facebook' social network Ello bec...   \n",
       "\n",
       "                                                link  \n",
       "1  https://www.reddit.com/r/tech/comments/2k4j3k/...  \n",
       "2  https://www.reddit.com/r/tech/comments/2k4j3k/...  \n",
       "3  https://www.reddit.com/r/tech/comments/2k4j3k/...  \n",
       "4  https://www.reddit.com/r/tech/comments/2k4j3k/...  \n",
       "5  https://www.reddit.com/r/tech/comments/2k4j3k/...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatedtopic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit.to_csv(\"comments1.csv\", encoding=\"utf-8\")\n",
    "relatedtopic.to_csv(\"comments2.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named colorbar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-027c8155d08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2334\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2255\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.pyc\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/magics/pylab.pyc\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3132\u001b[0;31m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3133\u001b[0m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/IPython/core/pylabtools.pyc\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'backend'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/victorialin/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pylab_helpers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named colorbar"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>body</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>Worker1</th>\n",
       "      <th>Answer1</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Date</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3L21G7IH4773YI2L6RV6T5G4RTC1YW</td>\n",
       "      <td>1199</td>\n",
       "      <td>I shall do that! I was thinking \"I've had this...</td>\n",
       "      <td>Fitbit</td>\n",
       "      <td>1449270750</td>\n",
       "      <td>3</td>\n",
       "      <td>A2R0YYUAWNT7UD</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-05 06:58:53 UTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3I7KR83SNAOQ3IGZ6P98Z3JCRQR9KI</td>\n",
       "      <td>1178</td>\n",
       "      <td>Love it. Has helped keep me on track for weigh...</td>\n",
       "      <td>Fitbit</td>\n",
       "      <td>1449265917</td>\n",
       "      <td>2</td>\n",
       "      <td>A1NM7ZPZ3NH412</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-05 06:59:05 UTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3P7RGTLO6EO481Q4YVN8VYUWX89KA8</td>\n",
       "      <td>1174</td>\n",
       "      <td>I love it because I do not have to stop and pu...</td>\n",
       "      <td>Fitbit</td>\n",
       "      <td>1449251995</td>\n",
       "      <td>2</td>\n",
       "      <td>A3ITZNJQUTIZ4C</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-05 07:02:17 UTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>373L46LKP7HF9UT8S10LOXXFNV3KJO</td>\n",
       "      <td>409</td>\n",
       "      <td>I never wore a watch, now I wear an Apple Watc...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>1449290608</td>\n",
       "      <td>1</td>\n",
       "      <td>A2R0YYUAWNT7UD</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-05 06:59:32 UTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3G9UA71JVV5REFMO97BCKSSTHB2J7G</td>\n",
       "      <td>1465</td>\n",
       "      <td>The fallout soundtracks are great, but IMO the...</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>1449249457</td>\n",
       "      <td>1</td>\n",
       "      <td>A1FP3SH704X01V</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-12-05 06:59:41 UTC</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Unnamed: 0  index  \\\n",
       "0  3L21G7IH4773YI2L6RV6T5G4RTC1YW   1199   \n",
       "1  3I7KR83SNAOQ3IGZ6P98Z3JCRQR9KI   1178   \n",
       "2  3P7RGTLO6EO481Q4YVN8VYUWX89KA8   1174   \n",
       "3  373L46LKP7HF9UT8S10LOXXFNV3KJO    409   \n",
       "4  3G9UA71JVV5REFMO97BCKSSTHB2J7G   1465   \n",
       "\n",
       "                                                body  company        date  \\\n",
       "0  I shall do that! I was thinking \"I've had this...   Fitbit  1449270750   \n",
       "1  Love it. Has helped keep me on track for weigh...   Fitbit  1449265917   \n",
       "2  I love it because I do not have to stop and pu...   Fitbit  1449251995   \n",
       "3  I never wore a watch, now I wear an Apple Watc...    Apple  1449290608   \n",
       "4  The fallout soundtracks are great, but IMO the...  Spotify  1449249457   \n",
       "\n",
       "   score         Worker1  Answer1  Avg                     Date positive  \n",
       "0      3  A2R0YYUAWNT7UD        2    2  2015-12-05 06:58:53 UTC     True  \n",
       "1      2  A1NM7ZPZ3NH412        2    2  2015-12-05 06:59:05 UTC     True  \n",
       "2      2  A3ITZNJQUTIZ4C        2    2  2015-12-05 07:02:17 UTC     True  \n",
       "3      1  A2R0YYUAWNT7UD        2    2  2015-12-05 06:59:32 UTC     True  \n",
       "4      1  A1FP3SH704X01V        2    2  2015-12-05 06:59:41 UTC     True  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_comments = pd.read_csv('comments1_with_sentimentsoriginal.csv')\n",
    "labeled_comments = labeled_comments.dropna(subset=['body', 'Answer1']) \n",
    "\n",
    "labeled_comments['positive'] = labeled_comments['Answer1'] >= 0\n",
    "labeled_comments['Answer1'] = labeled_comments['Answer1'].apply(int)\n",
    "\n",
    "labeled_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name stem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-92ffd1781367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPORTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'[.,;:!?]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name stem"
     ]
    }
   ],
   "source": [
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "import re\n",
    "\n",
    "punctuation = r'[.,;:!?]'\n",
    "\n",
    "def num_sentence(body):\n",
    "    return len(re.split(punctuation, body))\n",
    "\n",
    "def num_word(sentence):\n",
    "    return len(re.split(\" \", sentence))\n",
    "\n",
    "num_word = labeled_comments.body.apply(num_word)\n",
    "num_sentence = labeled_comments.body.apply(num_sentence)\n",
    "labeled_comments['num_word'] = num_word\n",
    "labeled_comments['num_sentence'] = num_sentence\n",
    "\n",
    "labeled_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-054b2edbcf6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Answer1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_comments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(labeled_comments['Answer1'])\n",
    "plt.hist(labeled_comments['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-859dbb857c3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mXbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_xyBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-859dbb857c3e>\u001b[0m in \u001b[0;36mmake_xy\u001b[0;34m(critics, vectorizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some versions of sklearn return COO format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "def make_xy(critics, vectorizer=None):\n",
    "    #Your code here    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(encoding = 'latin-1', strip_accents = 'ascii', stop_words='english')\n",
    "    X = vectorizer.fit_transform(critics.body)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (critics.Answer1).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_xyBinary(critics, vectorizer=None):\n",
    "    #Your code here    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(encoding = 'latin-1', strip_accents = 'ascii', stop_words='english')\n",
    "    X = vectorizer.fit_transform(critics.body)\n",
    "    X = X.tocsc()  # some versions of sklearn return COO format\n",
    "    y = (critics.positive).values.astype(np.int)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_xy(labeled_comments)\n",
    "Xbin, ybin = make_xyBinary(labeled_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train/test split the data\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X, y)\n",
    "xbintrain, xbintest, ybintrain, ybintest = train_test_split(Xbin, ybin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define cross-validation function\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    for train, test in KFold(y.size, nfold): # split data into train/test groups, 5 times\n",
    "        clf.fit(X[train], y[train]) # fit\n",
    "        result += scorefunc(clf, X[test], y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    prob = clf.predict_log_proba(x)\n",
    "    neg = y < 0\n",
    "    pos = ~neg\n",
    "    return prob[neg, 0].sum() + prob[pos, 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "itrain, itest = train_test_split(xrange(labeled_comments.shape[0]), train_size=0.7)\n",
    "mask=np.ones(labeled_comments.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in xrange(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "print \"Naive Bayes\"\n",
    "print \"Accuracy on training data: %0.2f\" % (clf.score(xtrain, ytrain))\n",
    "print \"MN Accuracy: %0.2f\" % (clf.score(xtest, ytest))\n",
    "\n",
    "clfbin = MultinomialNB()\n",
    "clfbin = clfbin.fit(xbintrain, ybintrain)\n",
    "print\n",
    "print \"Naive Bayes Binary\"\n",
    "print \"Accuracy on training data: %0.2f\" % (clfbin.score(xbintrain, ybintrain))\n",
    "print \"MN Accuracy: %0.2f\" % (clfbin.score(xbintest, ybintest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(clf, binary= False):\n",
    "    alphas = [0, .1, 1, 5, 10, 50]\n",
    "    min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "\n",
    "    #Find the best value for alpha and min_df, and the best classifier\n",
    "    best_alpha = None\n",
    "    best_min_df = None\n",
    "    maxscore=-np.inf\n",
    "    for alpha in alphas:\n",
    "        for min_df in min_dfs:         \n",
    "            vectorizer = CountVectorizer(min_df = min_df, encoding = 'latin-1')\n",
    "            if not binary:\n",
    "                Xthis, ythis = make_xy(critics, vectorizer)\n",
    "            else:\n",
    "                Xthis, ythis = make_xyBinary(critics,vectorizer)\n",
    "            Xtrainthis=Xthis[mask]\n",
    "            ytrainthis=ythis[mask]\n",
    "            #your code here\n",
    "            clf = MultinomialNB(alpha=alpha)\n",
    "            cvscore = cv_score(clf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "            if cvscore > maxscore:\n",
    "                maxscore = cvscore\n",
    "                best_alpha, best_min_df = alpha, min_df\n",
    "    return best_alpha, best_min_df\n",
    "\n",
    "best_alpha, best_min_df = validation(clf)\n",
    "best_alpha_bin, best_min_df_bin = validation(clf, binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df, encoding = 'latin-1')\n",
    "X1, y1 = make_xy(labeled_comments, vectorizer)\n",
    "xtrain1=X1[mask]\n",
    "ytrain1=y1[mask]\n",
    "xtest1=X1[~mask]\n",
    "ytest1=y1[~mask]\n",
    "\n",
    "best_clf = MultinomialNB(alpha=best_alpha).fit(xtrain1, ytrain1)\n",
    "\n",
    "print \"Optimized Naive Bayes\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_clf.score(xtrain1, ytrain1))\n",
    "print \"Accuracy on test data:     %0.2f\" % (best_clf.score(xtest1, ytest1))\n",
    "\n",
    "vectorizerbin = CountVectorizer(min_df=best_min_df_bin, encoding='latin-1')\n",
    "Xbin, ybin = make_xyBinary(labeled_comments, vectorizer)\n",
    "xbintrain1=Xbin[mask]\n",
    "ybintrain1=ybin[mask]\n",
    "xbintest1=Xbin[~mask]\n",
    "ybintest1=ybin[~mask]\n",
    "\n",
    "best_clfbin = MultinomialNB(alpha=best_alpha_bin).fit(xbintrain1, ybintrain1)\n",
    "\n",
    "print\n",
    "print \"Optimized Naive Bayes with Binary Data\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_clfbin.score(xbintrain1, ybintrain1))\n",
    "print \"Accuracy on test data:     %0.2f\" % (best_clfbin.score(xbintest1, ybintest1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(palette=\"muted\"):\n",
    "    roc_plot1 = make_roc(\"multinomial-nb\", best_clfbin, ybintest1, xbintest1, labe=200, skip=50)\n",
    "    roc_plot1.set_title(\"ROC curve for Naive Bayes Binary Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, n_jobs=-1, class_weight = 'auto').fit(xtrain,ytrain)\n",
    "print \"Random Forest with Ranges\"\n",
    "print \"Accuracy on training data: %0.2f\" % (rf.score(xtrain, ytrain))\n",
    "print \"RF Accuracy: %.02f\" % (rf.score(xtest,ytest))\n",
    "\n",
    "rfbin = RandomForestClassifier(n_estimators=100, min_samples_split=2, n_jobs= -1, class_weight = 'auto').fit(xbintrain,ybintrain)\n",
    "print \"Random Forest Binary Data\"\n",
    "print \"Accuracy on training data: %0.2f\" % (rfbin.score(xbintrain, ybintrain))\n",
    "print \"RF Accuracy: %.02f%%\" % (100*rfbin.score(xbintest,ybintest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_ests = [10, 20, 50, 100, 150, 200]\n",
    "min_dfs = [0,.1,.01,.001,.0001,1]\n",
    "max_depths = [5,10,15,20,25,40,50,70,80,90]\n",
    "best_n_est = 10\n",
    "\n",
    "maxscore=-np.inf\n",
    "for max_depth in max_depths:\n",
    "    for n_est in n_ests:  \n",
    "        for min_df in min_dfs:\n",
    "            vectorizer = CountVectorizer(min_df = min_df, encoding = 'latin-1')       \n",
    "            Xthis, ythis = make_xy(labeled_comments, vectorizer)\n",
    "            Xtrainthis=Xthis[mask]\n",
    "            ytrainthis=ythis[mask]\n",
    "            rf = RandomForestClassifier(n_estimators=n_est,max_depth = max_depth, min_samples_split=2, class_weight = 'auto', n_jobs=-1)\n",
    "            cvscore = cv_score(rf, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "            if cvscore > maxscore:\n",
    "                maxscore = cvscore\n",
    "                best_n_est, best_min_df, best_max_depth = n_est, min_df, max_depth\n",
    "                \n",
    "maxscore_bin=-np.inf\n",
    "for max_depth in max_depths:\n",
    "    for n_est in n_ests:  \n",
    "        for min_df in min_dfs:\n",
    "            vectorizer = CountVectorizer(min_df = min_df, encoding = 'latin-1')       \n",
    "            Xthis_bin, ythis_bin = make_xyBinary(labeled_comments, vectorizer)\n",
    "            Xtrainthis_bin=Xthis_bin[mask]\n",
    "            ytrainthis_bin=ythis_bin[mask]\n",
    "            rfbin = RandomForestClassifier(n_estimators=n_est,max_depth = max_depth, min_samples_split=2, class_weight = 'auto', n_jobs=-1)\n",
    "            cvscore = cv_score(rfbin, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "            if cvscore > maxscore_bin:\n",
    "                maxscore_bin = cvscore\n",
    "                best_n_est_bin, best_min_df_bin, best_max_depth_bin = n_est, min_df, max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df, encoding = 'latin-1')\n",
    "X2, y2 = make_xy(labeled_comments, vectorizer)\n",
    "xtrain2=X2[mask]\n",
    "ytrain2=y2[mask]\n",
    "xtest2=X2[~mask]\n",
    "ytest2=y2[~mask]\n",
    "\n",
    "best_rf = RandomForestClassifier(n_estimators=best_n_est, max_depth=best_max_depth, min_samples_split=2, n_jobs=-1, class_weight= 'auto').fit(xtrain2,ytrain2)\n",
    "\n",
    "print \"Optimized Random Forest with Ranges\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_rf.score(xtrain2, ytrain2))\n",
    "print \"Accuracy on test %0.2f\" % (best_rf.score(xtest2, ytest2))\n",
    "\n",
    "vectorizerbin = CountVectorizer(min_df=best_min_df_bin)\n",
    "Xbin2, ybin2 = make_xyBinary(labeled_comments, vectorizer)\n",
    "xbintrain2=Xbin2[mask]\n",
    "ybintrain2=ybin2[mask]\n",
    "xbintest2=Xbin[~mask]\n",
    "ybintest2=ybin[~mask]\n",
    "\n",
    "best_rfbin = RandomForestClassifier(n_estimators=best_n_est_bin, max_depth=best_max_depth_bin, min_samples_split=2, n_jobs=-1, class_weight= 'auto').fit(xbintrain2,ybintrain2)\n",
    "\n",
    "print \"Optimized Random Forest with Binary Data\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_rfbin.score(xbintrain2, ybintrain2))\n",
    "print \"RF Accuracy: %.02f%%\" % (best_rfbin.score(xbintest2, ybintest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(palette=\"muted\"):\n",
    "    roc_curve2 = make_roc(\"random-forest\", best_rfbin, ybintest2, xbintest2, labe=200, skip=50)\n",
    "    roc_curve2.set_title(\"ROC curve for Random Forest Binary Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "logistic = LogisticRegression(penalty = 'l1', class_weight = 'auto')\n",
    "logistic = logistic.fit(xbintrain, ybintrain)\n",
    "print \"Logistic Regression\"\n",
    "print \"Accuracy on training data: %0.2f\" % (logistic.score(xbintrain,ybintrain))\n",
    "print \"Logistic Accuracy: %.02f\" % (logistic.score(xbintest,ybintest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "min_dfs = [ 1e-4, 1e-3, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for c in Cs:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(min_df = min_df)\n",
    "        Xthis, ythis = make_xyBinary(labeled_comments, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        logistic = LogisticRegression(penalty = 'l1', C=c, class_weight='auto')\n",
    "        cvscore = cv_score(logistic, Xtrainthis, ytrainthis, log_likelihood)\n",
    "\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_c, best_min_df = c, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df, encoding= 'latin-1')\n",
    "X3, y3 = make_xyBinary(labeled_comments, vectorizer)\n",
    "xbintrain3=X3[mask]\n",
    "ybintrain3=y3[mask]\n",
    "xbintest3=X3[~mask]\n",
    "ybintest3=y3[~mask]\n",
    "\n",
    "best_log = LogisticRegression(penalty = 'l1', C=best_c, class_weight='auto').fit(xbintrain3,ybintrain3)\n",
    "\n",
    "print \"Optimized Logistic Regression\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_log.score(xbintrain3, ybintrain3))\n",
    "print \"Accuracy on test %0.2f\" % (best_log.score(xbintest3, ybintest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(palette=\"muted\"):\n",
    "    roc_curve3 = make_roc(\"logistic\", best_log, ybintest3, xbintest3, labe=200, skip=50)\n",
    "    roc_curve3.set_title(\"ROC curve for Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVM\n",
    "clfsvm = SVC(kernel='linear', class_weight='auto').fit(xtrain, ytrain)\n",
    "print \"Support Vector Machine\"\n",
    "print \"Accuracy on training data: %0.2f\" % (clfsvm.score(xtest, ytest))\n",
    "print \"Logistic Accuracy: %.02f\" % (clfsvm.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = [.001,.01, .1, 1, 10, 100]\n",
    "min_dfs = [1e-4, 1e-3, 1e-1]\n",
    "\n",
    "#Find the best value for alpha and min_df, and the best classifier\n",
    "best_alpha = None\n",
    "best_min_df = None\n",
    "maxscore=-np.inf\n",
    "for c in Cs:\n",
    "    for min_df in min_dfs:         \n",
    "        vectorizer = CountVectorizer(min_df = min_df, encoding = 'latin-1')\n",
    "        Xthis, ythis = make_xyBinary(labeled_comments, vectorizer)\n",
    "        Xtrainthis=Xthis[mask]\n",
    "        ytrainthis=ythis[mask]\n",
    "        svm = SVC(kernel='linear', C=c, class_weight='auto', probability=True)\n",
    "        cvscore = cv_score(svm, Xtrainthis, ytrainthis, log_likelihood)\n",
    "        if cvscore > maxscore:\n",
    "            maxscore = cvscore\n",
    "            best_c, best_min_df = c, min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=best_min_df)\n",
    "Xbin4, ybin4 = make_xyBinary(labeled_comments, vectorizer)\n",
    "xbintrain4=Xbin4[mask]\n",
    "ybintrain4=ybin4[mask]\n",
    "xbintest4=Xbin4[~mask]\n",
    "ybintest4=ybin4[~mask]\n",
    "\n",
    "best_svm = SVC(kernel='linear', C=best_c, class_weight ='auto').fit(xbintrain4,ybintrain4)\n",
    "\n",
    "print \"Optimized SVM Parameters\"\n",
    "print \"Accuracy on training data: %0.2f\" % (best_svm.score(xbintrain4, ybintrain4))\n",
    "print \"Accuracy on test %0.2f\" % (best_svm.score(xbintest4, ybintest4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(palette=\"muted\"):\n",
    "    roc_curve4 = make_roc(\"linear-svm\", best_svm, ybintest4, xbintest4, proba=False, labe=200, skip=50)\n",
    "    roc_curve.set_title(\"ROC curve for Linear SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reddit = []\n",
    "for row in range(len(labeled_comments.body)):\n",
    "    words = critics.iloc[row].body.split()\n",
    "    temp = (words, critics.iloc[row].Answer1)\n",
    "    reddit.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_words(reddits):\n",
    "    all_words = []\n",
    "    for (a, b) in reddits:\n",
    "        all_words.extend(a)\n",
    "    return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "word_features = get_word_features(get_words(reddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = nltk.classify.apply_features(extract_features, reddit)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print 'Accuracy:', nltk.classify.util.accuracy(classifier, training_set)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sns.color_palette(palette=\"muted\"):\n",
    "    roc_plot = make_roc(\"multinomial-nb\", best_clfbin, ybintest1, xbintest1, labe=200, skip=50)\n",
    "    make_roc(\"random-forest\", best_rfbin, ybintest2, xbintest2, labe=200, ax=roc_plot, skip=50)\n",
    "    make_roc(\"logistic\", best_log, ybintest3, xbintest3, labe=200, ax=roc_plot, skip=50)\n",
    "    make_roc(\"linear-svm\", best_svm, ybintest4, xbintest4, proba=False, labe=200, ax=roc_plot, skip=50)\n",
    "    roc_plot.set_title(\"ROC curve comparison of models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calibration_plot(clf, xtest, ytest):\n",
    "    prob = clf.predict_proba(xtest)[:, 1]\n",
    "    outcome = ytest\n",
    "    data = pd.DataFrame(dict(prob=prob, outcome=outcome))\n",
    "\n",
    "    #group outcomes into bins of similar probability\n",
    "    bins = np.linspace(0, 1, 20)\n",
    "    cuts = pd.cut(prob, bins)\n",
    "    binwidth = bins[1] - bins[0]\n",
    "    \n",
    "    #freshness ratio and number of examples in each bin\n",
    "    cal = data.groupby(cuts).outcome.agg(['mean', 'count'])\n",
    "    cal['pmid'] = (bins[:-1] + bins[1:]) / 2\n",
    "    cal['sig'] = np.sqrt(cal.pmid * (1 - cal.pmid) / cal['count'])\n",
    "        \n",
    "    #the calibration plot\n",
    "    ax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "    p = plt.errorbar(cal.pmid, cal['mean'], cal['sig'])\n",
    "    plt.plot(cal.pmid, cal.pmid, linestyle='--', lw=1, color='k')\n",
    "    plt.ylabel(\"Empirical P(+)\")\n",
    "    \n",
    "    #the distribution of P(fresh)\n",
    "    ax = plt.subplot2grid((3, 1), (2, 0), sharex=ax)\n",
    "    \n",
    "    plt.bar(left=cal.pmid - binwidth / 2, height=cal['count'],\n",
    "            width=.95 * (bins[1] - bins[0]),\n",
    "            fc=p[0].get_color())\n",
    "    \n",
    "    plt.xlabel(\"Predicted P(_)\")\n",
    "    plt.ylabel(\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(best_clfbin, xbintest1, ybintest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(best_rfbin, xbintest2, ybintest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(best_log, xbintest3, ybintest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calibration_plot(best_svm, xbintest4, ybintest4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-bc9e70523fee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munlabeled_comments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comments.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mXbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_xyBinary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabeled_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-859dbb857c3e>\u001b[0m in \u001b[0;36mmake_xy\u001b[0;34m(critics, vectorizer)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# some versions of sklearn return COO format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "unlabeled_comments = pd.read_csv(\"comments.csv\")\n",
    "\n",
    "X, y = make_xy(unlabeled_comments)\n",
    "Xbin, ybin = make_xyBinary(unlabeled_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction without Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = best_log.predict(Xbin)\n",
    "unlabeled_comments[\"prediction_binary\"] = prediction\n",
    "unlabeled_comments['prediction'] = prediction*unlabeled_comments['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company = unlabeled_comments.groupby('company').agg({'prediction':np.mean})\n",
    "company2 = unlabeled_comments.groupby('company').agg({'prediction_binary':np.mean})\n",
    "\n",
    "names = []\n",
    "for a,b in grp:\n",
    "    names.append(a)\n",
    "    \n",
    "values = []\n",
    "for i in company.values:\n",
    "    values.append(i[0])\n",
    "    \n",
    "values2 = []\n",
    "for i in company2.values:\n",
    "    values2.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "final['weighted_predict'] = values\n",
    "final['name'] = names\n",
    "final['predict_binary'] = values2\n",
    "\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.to_csv(\"company_with_predictions_binary_no_hierarchy.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = best_rf.predict(X)\n",
    "unlabeled_comments['prediction_range'] = prediction\n",
    "unlabeled_comments['prediction_range'] = prediction*unlabeled_comments['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_range = unlabeled_comments.groupby('company').agg({'prediction_range':np.mean})\n",
    "\n",
    "weight_values = []\n",
    "for i in company_range.values:\n",
    "    weight_values.append(i[0])\n",
    "    \n",
    "final_weight = pd.DataFrame()\n",
    "final_weight['weighted_predict_range'] = weight_values\n",
    "final_weight['name'] = names\n",
    "\n",
    "final_weight.to_csv(\"company_with_prediction_range_no_hierarchy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Text Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm, ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_model = ols('Answer1 ~prediction_range_range + num_word:num_sentence + num_word ', labeled_comments).fit()\n",
    "ols_model.summary()\n",
    "\n",
    "predictions = ols_model.predict(unlabeled_comments)\n",
    "\n",
    "unlabeled_comments['prediction_range'] = y*unlabeled_comments['score']\n",
    "unlabeled_comments['range'] = predictions\n",
    "\n",
    "company_range = unlabeled_comments.groupby('company').agg({'prediction_range':np.mean})\n",
    "company_range2 = unlabeled_comments.groupby('company').agg({'range':np.mean})\n",
    "\n",
    "weight_values = []\n",
    "for i in company_range.values:\n",
    "    weight_values.append(i[0])\n",
    "    \n",
    "weight = []\n",
    "for i in company_range2.values:\n",
    "    weight.append(i[0])\n",
    "final_weight = pd.DataFrame()\n",
    "\n",
    "final_weight['weighted_predict_range'] = weight_values\n",
    "final_weight['name'] = names\n",
    "final_weight['predict_range'] = weight\n",
    "\n",
    "final_weight.to_csv(\"company_with_prediction_range_multimodel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Hierarchical Prediction of Startup Funding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data file downloaded from Crunchbase database\n",
    "crunchbasedf = pd.read_csv('crunchbase_export.csv')\n",
    "\n",
    "# import list of companies that we did sentiment analysis on\n",
    "companies_list = pd.read_csv('companies.csv').company.tolist()\n",
    "\n",
    "# make all company names lowercase for easier comparison\n",
    "companies_list = [x.lower() for x in companies_list]\n",
    "lowercase = lambda x: str(x).lower()\n",
    "crunchbasedf.name = crunchbasedf.name.apply(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create formatter that changes string into proper format \n",
    "# (for some reason, the data in crunchbasedf is formatted \n",
    "# really weirdly, requiring me to do this)\n",
    "\n",
    "def formatter(string):\n",
    "    space = False\n",
    "    formatted = \"\"\n",
    "    for s in string:\n",
    "        if space and (s is not ' '):\n",
    "            formatted += s\n",
    "        elif s == ' ':\n",
    "            space = True\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ************************************************ #\n",
    "# CREATE DATAFRAME WITH ONLY THE COMPANIES WE NEED #\n",
    "# ************************************************ #\n",
    "\n",
    "# declare lists of relevant demographics data we will be using\n",
    "first_funding_date = []\n",
    "last_funding_date = []\n",
    "funding_total = []\n",
    "founded_date = []\n",
    "public = []\n",
    "funding_rounds = []\n",
    "new_companies_list = []\n",
    "\n",
    "# iterate through list of companies to extract information for each companies\n",
    "for company in companies_list:\n",
    "    # check if company is in crunchbasedf\n",
    "    if company in crunchbasedf.name.tolist():\n",
    "        # only include companies in crunchbasedf\n",
    "        new_companies_list.append(company)\n",
    "        \n",
    "        # create public variable\n",
    "        if formatter(crunchbasedf[crunchbasedf.name==company].status.to_string()).strip() == \"ipo\":\n",
    "            public.append(1)\n",
    "        else:\n",
    "            public.append(0)\n",
    "\n",
    "        # create funding rounds variable\n",
    "        funding_rounds.append(int(formatter(crunchbasedf[crunchbasedf.name==company].funding_rounds.to_string()).strip()))\n",
    "\n",
    "        # create total funds variable\n",
    "        funding_total.append(int(formatter(crunchbasedf[crunchbasedf.name==company][' funding_total_usd '].to_string()).replace(\",\",\"\").strip()))\n",
    "\n",
    "        # create last funding date variable\n",
    "        last_funding_date.append(crunchbasedf[crunchbasedf.name==company].last_funding_at.to_string()[-10:])\n",
    "\n",
    "        # create first funding date variable\n",
    "        first_funding_date.append(crunchbasedf[crunchbasedf.name==company].first_funding_at.to_string()[-10:])\n",
    "\n",
    "        # create founded date variable\n",
    "        founded_date.append(crunchbasedf[crunchbasedf.name==company].founded_at.to_string()[-10:])\n",
    "\n",
    "# create dictionary with all \n",
    "dfdict = {'company':new_companies_list,'first_funding_date':first_funding_date,'last_funding_date':last_funding_date,\n",
    "          'funding_total':funding_total,'founded_date':founded_date,'public':public,'funding_rounds':funding_rounds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert dictionary \n",
    "infodf = pd.DataFrame.from_dict(dfdict)\n",
    "\n",
    "# added founded date into Vivint through other source, since Cruchbase didn't have it\n",
    "infodf.loc[33,'founded_date'] = '1997-06-01'\n",
    "\n",
    "# change strings into date items to calculate company age and time since last funding\n",
    "infodf.first_funding_date = infodf.first_funding_date.apply(pd.datetools.parse)\n",
    "infodf.last_funding_date = infodf.last_funding_date.apply(pd.datetools.parse)\n",
    "infodf.founded_date = infodf.founded_date.apply(pd.datetools.parse)\n",
    "\n",
    "infodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate age and time since last funding\n",
    "import datetime\n",
    "today = datetime.datetime(2015, 12, 10)\n",
    "infodf['age'] = (today - infodf['founded_date']).values / np.timedelta64(1, 'D')\n",
    "infodf['time_from_last_funding'] = (today - infodf['last_funding_date']).values / np.timedelta64(1, 'D')\n",
    "infodf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert csv's with sentiment predictions into pandas dataframes\n",
    "bin_pred = pd.read_csv('company_with_predictions.csv')\n",
    "range_pred = pd.read_csv('company_with_prediction_range.csv')\n",
    "\n",
    "# drop first column (index column)\n",
    "bin_pred = bin_pred.drop(bin_pred.columns[[0]], axis=1)\n",
    "range_pred = range_pred.drop(range_pred.columns[[0]], axis=1)\n",
    "\n",
    "# change name of column from 'name' to 'company' to allow dataframe to merge with infodf in next step\n",
    "bin_pred = bin_pred.rename(columns={'name': 'company'})\n",
    "range_pred = range_pred.rename(columns={'name': 'company'})\n",
    "\n",
    "# make company names lowercase to allow for successful merge with infodf\n",
    "bin_pred.company = bin_pred.company.apply(lowercase)\n",
    "range_pred.company = range_pred.company.apply(lowercase)\n",
    "bin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge sentiment data with demographics data\n",
    "newdf1 = pd.merge(infodf,bin_pred,how='inner',on=['company'])\n",
    "newdf2 = pd.merge(infodf,range_pred,how='inner',on=['company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "newdf1 = newdf1.drop(['company','first_funding_date','founded_date','last_funding_date',], 1)\n",
    "newdf2 = newdf2.drop(['company','first_funding_date','founded_date','last_funding_date',], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some EDA to see the distribution of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of total funding\n",
    "plt.hist(newdf1.funding_total,bins=20)\n",
    "plt.title(\"Total Funding of Companies\")\n",
    "plt.xlabel(\"Total Funding (in millions $)\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of number of funding rounds\n",
    "plt.hist(newdf1.funding_rounds,bins=10)\n",
    "plt.title(\"Funding Rounds of Companies\")\n",
    "plt.xlabel(\"Number of Funding Rounds\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of age of companies\n",
    "plt.hist(newdf1.age,bins=15)\n",
    "plt.title(\"Age of Companies (in days)\")\n",
    "plt.xlabel(\"Age (days)\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of time since last funding\n",
    "plt.hist(newdf1.time_from_last_funding,bins=15)\n",
    "plt.title(\"Time Since Last Funding for All Companies\")\n",
    "plt.xlabel(\"Time Since Last Funding\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of weighted binary sentiment ranking\n",
    "plt.hist(newdf1.weighted_predict,bins=20)\n",
    "plt.title(\"Predicted Weighted Binary Sentiment Ranking\")\n",
    "plt.xlabel(\"Weighted Binary Sentiment Prediction\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of weighted sentiment rank from -2 to 2\n",
    "plt.hist(newdf2.weighted_predict_range,bins=20)\n",
    "plt.title(\"Weighted Predicted Sentiment Ranking (on -2 to 2 scale)\")\n",
    "plt.xlabel(\"Sentiment Prediction (from -2 to 2)\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of binary sentiment ranking without weighting\n",
    "plt.hist(newdf1.predict_binary,bins=20)\n",
    "plt.title(\"Predicted Nonweighted Binary Sentiment Ranking\")\n",
    "plt.xlabel(\"Binary Sentiment Prediction\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of sentiment ranking from -2 to 2 without weighting\n",
    "plt.hist(newdf2.predict_range,bins=20)\n",
    "plt.title(\"Predicted Nonweighted Sentiment Ranking (from -2 to 2)\")\n",
    "plt.xlabel(\"Predicted Sentiment Ranking (from -2 to 2)\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try creating a linear regression model. First we try an ordinary least squares regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_model1 = ols('funding_total ~ time_from_last_funding + age + public + funding_rounds + weighted_predict + predict_binary', newdf1).fit()\n",
    "ols_model2 = ols('funding_total ~ time_from_last_funding + age + public + funding_rounds + weighted_predict_range + predict_range', newdf2).fit()\n",
    "ols_model1.summary()\n",
    "ols_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our R^2 values are really poor, we will try using a forward stepwise regression. Code is from http://planspace.org/20150423-forward_selection_with_statsmodels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ols_for_model1 = forward_selected(newdf1, 'funding_total')\n",
    "ols_for_model2 = forward_selected(newdf2, 'funding_total')\n",
    "ols_for_model1.summary()\n",
    "ols_for_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a significant improvement, although the R^2 values are better. Because some of the data is extremely right skew and the linear regression is still doing a poor job, let's transform the skew data and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logging the skew data (based on the histograms from above)\n",
    "newdf1['log_age'] = newdf1.age.apply(np.log)\n",
    "newdf1['log_time_from_last_funding'] =  newdf1.time_from_last_funding.apply(np.log)\n",
    "newdf2['log_age'] = newdf2.age.apply(np.log)\n",
    "newdf2['log_time_from_last_funding'] =  newdf2.time_from_last_funding.apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of log(age)\n",
    "plt.hist(newdf1.log_age,bins=15)\n",
    "plt.title(\"Age of Companies (in log(days))\")\n",
    "plt.xlabel(\"Age (log(days))\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of log of time since last funding\n",
    "plt.hist(newdf1.log_time_from_last_funding,bins=15)\n",
    "plt.title(\"Time Since Last Funding for All Companies (log plot)\")\n",
    "plt.xlabel(\"log(Time Since Last Funding)\")\n",
    "plt.ylabel(\"Number of Companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_newdf1 = newdf1.drop(['age','time_from_last_funding'], 1)\n",
    "log_newdf2 = newdf2.drop(['age','time_from_last_funding'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try forward selection on log data\n",
    "log_ols_for_model1 = forward_selected(log_newdf1, 'funding_total')\n",
    "log_ols_for_model2 = forward_selected(log_newdf2, 'funding_total')\n",
    "log_ols_for_model1.summary()\n",
    "log_ols_for_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this didn't do anything to improve our model, let's try using a different method. Here we will use a Random Forest Regressor and see how well it predicts the total funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = log_newdf1.funding_total\n",
    "X1 = log_newdf1.drop(['funding_total'],1)\n",
    "X2 = log_newdf2.drop(['funding_total'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()\n",
    "forest_model1 = model.fit(X1,y)\n",
    "forest_model2 = model.fit(X2,y)\n",
    "\n",
    "forest_model1.score(X1,y)\n",
    "forest_model2.score(X2,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our R^2 value has significantly improved. At this time, we cannot determine whether this is simply due to overfitting, since we do not have enough data to split between testing and training value. However, this does show promise for having potential predictive power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
